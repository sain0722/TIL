{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "- Torch의 Tensor\n",
    "- Tensor의 선언\n",
    "- Numpy to Tensor\n",
    "- Tensor to Numpy\n",
    "- view\n",
    "- cat\n",
    "- mean, sum\n",
    "- GPU 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.0062e+22, 4.5559e-41, 6.0735e-25])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.Tensor(3, 3)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6150, 0.2878, 0.4705],\n",
       "        [0.8970, 0.8634, 0.4327],\n",
       "        [0.1242, 0.2178, 0.3101]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor의 Random 선언\n",
    "a = torch.rand(3, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1560,  0.7746,  1.0326],\n",
       "        [-1.1882, -0.5802, -0.1564],\n",
       "        [-0.5510,  1.0210, -0.0040]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal distribution random 값으로 선언\n",
    "a = torch.randn(3, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy 에서 Tensor 로\n",
    "import numpy as np\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = torch.Tensor(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08342022, 0.1665942 , 0.17174852],\n",
       "       [0.15058577, 0.7105323 , 0.85587627],\n",
       "       [0.35589147, 0.22164148, 0.6660076 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor 에서 Numpy 로\n",
    "a = torch.rand(3, 3)\n",
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[0.3393, 0.7249, 0.5822],\n",
      "          [0.1551, 0.2113, 0.1342],\n",
      "          [0.1564, 0.0822, 0.0653]]]])\n"
     ]
    }
   ],
   "source": [
    "# Tensor의 형태 변환 (view)\n",
    "\n",
    "a = torch.rand(3, 3)\n",
    "a = a.view(1, 1, 3, 3)\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.4087, -0.3836, -0.1997],\n",
       "          [ 0.9159, -0.0943, -2.0407],\n",
       "          [-0.3011, -0.1097,  0.8523]]],\n",
       "\n",
       "\n",
       "        [[[-1.4659,  0.1223, -1.3216],\n",
       "          [-1.2374, -0.1963,  0.2494],\n",
       "          [-0.8658, -1.2885,  1.4062]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor 합치기\n",
    "# torch.cat((Tensor_A, Tensor_B), dim)\n",
    "\n",
    "a = torch.randn(1, 1, 3, 3)\n",
    "b = torch.randn(1, 1, 3, 3)\n",
    "c = torch.cat((a, b), 0)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6235, 0.8782, 1.0138],\n",
      "        [0.9935, 0.9642, 1.2879],\n",
      "        [1.0337, 1.7549, 1.0651]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Tensor 계산을 GPU로\n",
    "\n",
    "x = torch.rand(3, 3)\n",
    "y = torch.rand(3, 3)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4239)\n",
      "tensor(3.8147)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3 ,3)\n",
    "print(a.mean())\n",
    "print(a.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Autograd와 Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto Gradient : 미분 값을 자동으로 계산해준다.  \n",
    "자동 계산을 위해 사용하는 변수 -> Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1233, 0.5094, 0.6977, 0.5297, 0.2541])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "a = torch.rand(5)\n",
    "a = Variable(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable\n",
    "- data : Tensor형태의 데이터가 담김\n",
    "- grad : Data가 거쳐온 layer에 대한 미분값이 축적됨\n",
    "    - Update를 위한 Gradient Descent Value가 저장되어있음\n",
    "- grad_fn : 미분값을 계산한 함수에 대한 정보\n",
    "    - 어떤 연산에 대한 backward를 진행을 했다. 라는 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# input으로 적용되는 a값같은 경우에는, requires_grad를 True로 지정을 해줘야 함\n",
    "# True로 지정을 하지 않는다면, grad 값을 필요로 하지 않는다고 생각\n",
    "a = Variable(a, requires_grad=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------a.data------\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "------a.grad------\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "------a.grad_fn------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"------a.data------\")\n",
    "print(a.data)\n",
    "print(\"------a.grad------\")\n",
    "print(a.grad)\n",
    "print(\"------a.grad_fn------\")\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "b = a + 2\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9.],\n",
      "        [9., 9.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = b ** 2\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = c.sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward?\n",
    "- a.grad의 값을 채워주는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------a.data------\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "------a.grad------\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "------a.grad_fn------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"------a.data------\")\n",
    "print(a.data)\n",
    "print(\"------a.grad------\")\n",
    "print(a.grad)\n",
    "print(\"------a.grad_fn------\")\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------b.data------\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "------b.grad------\n",
      "None\n",
      "------b.grad_fn------\n",
      "<AddBackward0 object at 0x00000235004564C8>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangmin\\anaconda3\\lib\\site-packages\\torch\\tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    }
   ],
   "source": [
    "print(\"------b.data------\")\n",
    "print(b.data)\n",
    "print(\"------b.grad------\")\n",
    "print(b.grad)\n",
    "print(\"------b.grad_fn------\")\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------c.data------\n",
      "tensor([[9., 9.],\n",
      "        [9., 9.]])\n",
      "------c.grad------\n",
      "None\n",
      "------c.grad_fn------\n",
      "<PowBackward0 object at 0x00000235160C0D48>\n"
     ]
    }
   ],
   "source": [
    "print(\"------c.data------\")\n",
    "print(c.data)\n",
    "print(\"------c.grad------\")\n",
    "print(c.grad)\n",
    "print(\"------c.grad_fn------\")\n",
    "print(c.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------out.data------\n",
      "tensor(36.)\n",
      "------out.grad------\n",
      "None\n",
      "------out.grad_fn------\n",
      "<SumBackward0 object at 0x0000023516A7C988>\n"
     ]
    }
   ],
   "source": [
    "print(\"------out.data------\")\n",
    "print(out.data)\n",
    "print(\"------out.grad------\")\n",
    "print(out.grad)\n",
    "print(\"------out.grad_fn------\")\n",
    "print(out.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "y = x ** 2\n",
    "z = y * 3\n",
    "print(z)\n",
    "grad = torch.Tensor([0.1, 1, 10])\n",
    "z.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------x.data------\n",
      "tensor([1., 1., 1.])\n",
      "------x.grad------\n",
      "tensor([ 0.6000,  6.0000, 60.0000])\n",
      "------x.grad_fn------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"------x.data------\")\n",
    "print(x.data)\n",
    "print(\"------x.grad------\")\n",
    "print(x.grad)\n",
    "print(\"------x.grad_fn------\")\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. nn & nn.functional\n",
    "\n",
    "- torch.nn.conv2d : weight, bias를 자체적으로 가지고 있음\n",
    "- torch.nn.functional.conv2d : input, weight 자체를 직접 넣어준다.\n",
    "    - 외부에서 만든 필터를 직접 만들어서 넣어주어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones(1, 1, 3, 3)\n",
    "filter = torch.ones(1, 1, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "input = Variable(input, requires_grad=True)\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = Variable(filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = F.conv2d(input, filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[9.]]]], grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MkldnnConvolutionBackward object at 0x0000023349137508>\n"
     ]
    }
   ],
   "source": [
    "print(out.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Variable(torch.ones(1, 1, 3, 3), requires_grad = True)\n",
    "\n",
    "filter = filter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MkldnnConvolutionBackward object at 0x0000023348347C08>\n",
      "tensor([[[[2., 2., 2.],\n",
      "          [2., 2., 2.],\n",
      "          [2., 2., 2.]]]])\n"
     ]
    }
   ],
   "source": [
    "out = F.conv2d(input, filter)\n",
    "out.backward()\n",
    "print(out.grad_fn)\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.1351, -0.0776,  0.0351],\n",
       "          [ 0.2246, -0.0431,  0.1012],\n",
       "          [-0.1868, -0.1229,  0.1610]]]], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.ones(1, 1, 3, 3)\n",
    "input = Variable(input, requires_grad=True)\n",
    "# input, output, cunnel size\n",
    "# 1채널짜리가 들어가서, 1채널이 나오는데, 컨볼루션을 연산하는 필터의 크기는 3x3이다.\n",
    "func = nn.Conv2d(1, 1, 3)\n",
    "func.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.1368]]]], grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "out = func(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.1351, -0.0776,  0.0351],\n",
      "          [ 0.2246, -0.0431,  0.1012],\n",
      "          [-0.1868, -0.1229,  0.1610]]]])\n"
     ]
    }
   ],
   "source": [
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.0376,  0.0690,  0.0785],\n",
      "          [ 0.0063, -0.0958,  0.0356],\n",
      "          [-0.1103, -0.3263,  0.2562]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 5 X 5\n",
    "# pretrained model 사용\n",
    "# weight값 직접 설정\n",
    "\n",
    "input = torch.ones(1, 1, 5, 5)\n",
    "input = Variable(input, requires_grad=True)\n",
    "\n",
    "filter = nn.Conv2d(1, 1, 3, bias=None)\n",
    "print(filter.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter.weight = torch.nn.Parameter(torch.ones(1, 1, 3, 3) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[2., 2., 2.],\n",
       "          [2., 2., 2.],\n",
       "          [2., 2., 2.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[18., 18., 18.],\n",
      "          [18., 18., 18.],\n",
      "          [18., 18., 18.]]]], grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "out = filter(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu, sigmoid, tanh, Max Pooling\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
